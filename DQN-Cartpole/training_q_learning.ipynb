{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN vs. DSQN for the CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import date\n",
    "from model import QNetwork, DSNN\n",
    "from agent import Agent, ReplayBuffer\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment specific parameters\n",
    "env_name = 'CartPole-v0'\n",
    "n_runs = 10\n",
    "n_evaluations = 100\n",
    "max_steps = 200\n",
    "num_episodes = 1000\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Directory result_2_20211018 to store the results in\n"
     ]
    }
   ],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('.')\n",
    "if not any('result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'result_' + str(result_id) + '_{}'.format(\n",
    "    str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "discount_factor = 0.999\n",
    "eps_start = 1.0\n",
    "eps_end = 0.05\n",
    "eps_decay = 0.999\n",
    "update_every = 4\n",
    "target_update_frequency = 100\n",
    "learning_rate = 0.001\n",
    "replay_memory_size = 4*10**4\n",
    "tau = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN Hyperparameters\n",
    "time_step = 1e-3\n",
    "simulation_time = 10\n",
    "alpha = 1\n",
    "beta = 1\n",
    "weight_scale = 1\n",
    "threshold = 0.1\n",
    "architecture = [4, 64, 64, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [random.getrandbits(32) for _ in range(n_runs)]\n",
    "\n",
    "np.save('seeds', seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smoothed_scores_dqn_all = []\n",
    "dqn_completion_after = []\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run))\n",
    "    seed = seeds[i_run]\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    policy_net = QNetwork(architecture, seed).to(device)\n",
    "    target_net = QNetwork(architecture, seed).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau)\n",
    "    \n",
    "    smoothed_scores, scores, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DQN_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    dqn_completion_after.append(best_average_after)\n",
    "    smoothed_scores_dqn_all.append(smoothed_scores)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot scores of individual runs\n",
    "for i in range(len(smoothed_scores_dqn_all)):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(smoothed_scores_dqn_all[i])\n",
    "    plt.ylim(0, 250)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(result_dir + '/training_dqn_{}.png'.format(i), dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (mean)\n",
    "best_smoothed_scores_dqn = [smoothed_scores_dqn_all[best_runs[0]],\n",
    "                            smoothed_scores_dqn_all[best_runs[1]],\n",
    "                            smoothed_scores_dqn_all[best_runs[2]],\n",
    "                            smoothed_scores_dqn_all[best_runs[3]],\n",
    "                            smoothed_scores_dqn_all[best_runs[4]],\n",
    "                            smoothed_scores_dqn_all[best_runs[5]],\n",
    "                            smoothed_scores_dqn_all[best_runs[6]],\n",
    "                            smoothed_scores_dqn_all[best_runs[7]],\n",
    "                            smoothed_scores_dqn_all[best_runs[8]],\n",
    "                            smoothed_scores_dqn_all[best_runs[9]]]\n",
    "mean_smoothed_scores_dqn = np.mean(best_smoothed_scores_dqn, axis=0)\n",
    "std_smoothed_scores = np.std(best_smoothed_scores_dqn, axis=0)\n",
    "\n",
    "avg_dqn_completion_after = np.mean([dqn_completion_after[best_runs[0]],\n",
    "                                dqn_completion_after[best_runs[1]],\n",
    "                                dqn_completion_after[best_runs[2]],\n",
    "                                dqn_completion_after[best_runs[3]],\n",
    "                                dqn_completion_after[best_runs[4]],\n",
    "                                dqn_completion_after[best_runs[5]],\n",
    "                                dqn_completion_after[best_runs[6]],\n",
    "                                dqn_completion_after[best_runs[7]],\n",
    "                                dqn_completion_after[best_runs[8]],\n",
    "                                dqn_completion_after[best_runs[9]]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dqn[0])), mean_smoothed_scores_dqn)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 97, axis=0), alpha=0.25)\n",
    "plt.vlines(avg_dqn_completion_after, 0, 250, 'C0')\n",
    "#plt.fill_between(range(len(smoothed_scores_dqn_all[0])), mean_smoothed_scores-std_smoothed_scores,\n",
    "#                 mean_smoothed_scores+std_smoothed_scores, alpha=0.25)\n",
    "plt.ylim(0, 250)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DQN_training.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSQN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSQNs without two-neurons-input encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run # 0\n",
      "Episode 1\tAverage Score: 33.00\t Epsilon: 0.97\r",
      "Episode 2\tAverage Score: 22.00\t Epsilon: 0.96\r",
      "Episode 3\tAverage Score: 19.67\t Epsilon: 0.94\r",
      "Episode 4\tAverage Score: 28.00\t Epsilon: 0.89\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akl-ma/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 44.20\t Epsilon: 0.05\n",
      "Episode 200\tAverage Score: 41.85\t Epsilon: 0.05\n",
      "Episode 300\tAverage Score: 38.64\t Epsilon: 0.05\n",
      "Episode 400\tAverage Score: 35.87\t Epsilon: 0.05\n",
      "Episode 500\tAverage Score: 35.94\t Epsilon: 0.05\n",
      "Episode 600\tAverage Score: 45.84\t Epsilon: 0.05\n",
      "Episode 700\tAverage Score: 48.64\t Epsilon: 0.05\n",
      "Episode 778\tAverage Score: 49.41\t Epsilon: 0.05\r"
     ]
    }
   ],
   "source": [
    "smoothed_scores_dsqn_all = []\n",
    "dsqn_completion_after = []\n",
    "simulation_time = 10\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run))\n",
    "    seed = seeds[i_run]\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    policy_net = DSNN(architecture, seed, alpha, beta, batch_size, threshold, simulation_time)\n",
    "    target_net = DSNN(architecture, seed, alpha, beta, batch_size, threshold, simulation_time)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau, SQN=True, quantization=False)\n",
    "\n",
    "    smoothed_scores, scores, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DSQN_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dsqn_all.append(smoothed_scores)\n",
    "    dsqn_completion_after.append(best_average_after)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized DSQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smoothed_scores_dsqn_quantized_all = []\n",
    "dsqn_quantized_completion_after = []\n",
    "simulation_time = 8\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run))\n",
    "    seed = seeds[i_run]\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    policy_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time)\n",
    "    target_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau, SQN=True, two_neurons=False,\n",
    "                  quantization=True)\n",
    "\n",
    "    smoothed_scores, scores, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DSQN_Loihi_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dsqn_quantized_all.append(smoothed_scores)\n",
    "    dsqn_quantized_completion_after.append(best_average_after)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_scores_dsqn_quantized_all = smoothed_scores_dsqn_all\n",
    "dsqn_quantized_completion_after = dsqn_completion_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time, two_neurons=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsqn_completion_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_net.weights = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = seeds[0]\n",
    "policy_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time, two_neurons=False)\n",
    "target_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time, two_neurons=False)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, 0, result_dir, seed, tau, SQN=True, two_neurons=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = policy_net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_weights = agent.quantize_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quant_weights = [q_w.tensor.float() for q_w in q_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quant_weights[0].requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quant_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = (1.8 + 1.8)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w = np.concatenate((weights[0].detach().numpy()[0], weights[0].detach().numpy()[1], weights[0].detach().numpy()[2], weights[0].detach().numpy()[3]))\n",
    "bins = np.arange(-1.8, 1.8, step)\n",
    "plt.hist(w, bins)\n",
    "plt.title('FP32 Weights')\n",
    "plt.savefig('weights_fp32.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.concatenate((quant_weights[0].detach().numpy()[0], quant_weights[0].detach().numpy()[1], quant_weights[0].detach().numpy()[2], quant_weights[0].detach().numpy()[3]), axis=0)\n",
    "bins = range(-128, 127)\n",
    "plt.hist(w, bins)\n",
    "plt.title('Quantized Weights')\n",
    "plt.savefig('weights_quantized.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.weights = quant_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_runs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smoothed_scores_dsqn = [smoothed_scores_dsqn_all[best_runs[0]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[1]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[2]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[3]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[4]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[5]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[6]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[7]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[8]],\n",
    "                             smoothed_scores_dsqn_all[best_runs[9]]]\n",
    "mean_smoothed_scores_dsqn = np.mean(best_smoothed_scores_dsqn, axis=0)\n",
    "\n",
    "avg_dsqn_completion_after = np.mean([dsqn_completion_after[best_runs[0]],\n",
    "                                dsqn_completion_after[best_runs[1]],\n",
    "                                dsqn_completion_after[best_runs[2]],\n",
    "                                dsqn_completion_after[best_runs[3]],\n",
    "                                dsqn_completion_after[best_runs[4]],\n",
    "                                dsqn_completion_after[best_runs[5]],\n",
    "                                dsqn_completion_after[best_runs[6]],\n",
    "                                dsqn_completion_after[best_runs[7]],\n",
    "                                dsqn_completion_after[best_runs[8]],\n",
    "                                dsqn_completion_after[best_runs[9]]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dsqn[0])), mean_smoothed_scores_dsqn)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 97, axis=0), alpha=0.25)\n",
    "\n",
    "plt.vlines(avg_dsqn_completion_after, 0, 250, 'C0')\n",
    "\n",
    "\n",
    "plt.ylim(0, 250)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DSQN_training.png', dpi=1000)\n",
    "plt.title('CartPole-v0 DSQN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Quantized DSQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_scores_dsqn_quantized_0 = np.load('result_23_2021416/smoothed_scores_DSQN_0.npy')\n",
    "smoothed_scores_dsqn_quantized_1 = np.load('result_23_2021416/smoothed_scores_DSQN_1.npy')\n",
    "smoothed_scores_dsqn_quantized_2 = np.load('result_23_2021416/smoothed_scores_DSQN_2.npy')\n",
    "smoothed_scores_dsqn_quantized_3 = np.load('result_23_2021416/smoothed_scores_DSQN_3.npy')\n",
    "smoothed_scores_dsqn_quantized_4 = np.load('result_23_2021416/smoothed_scores_DSQN_4.npy')\n",
    "smoothed_scores_dsqn_quantized_5 = np.load('result_23_2021416/smoothed_scores_DSQN_5.npy')\n",
    "smoothed_scores_dsqn_quantized_6 = np.load('result_23_2021416/smoothed_scores_DSQN_6.npy')\n",
    "smoothed_scores_dsqn_quantized_7 = np.load('result_23_2021416/smoothed_scores_DSQN_7.npy')\n",
    "smoothed_scores_dsqn_quantized_8 = np.load('result_23_2021416/smoothed_scores_DSQN_8.npy')\n",
    "smoothed_scores_dsqn_quantized_9 = np.load('result_23_2021416/smoothed_scores_DSQN_9.npy')\n",
    "smoothed_scores_dsqn_quantized_all = [smoothed_scores_dsqn_quantized_0, smoothed_scores_dsqn_quantized_1, smoothed_scores_dsqn_quantized_2, smoothed_scores_dsqn_quantized_3, smoothed_scores_dsqn_quantized_4, smoothed_scores_dsqn_quantized_5, smoothed_scores_dsqn_quantized_6, smoothed_scores_dsqn_quantized_7, smoothed_scores_dsqn_quantized_8, smoothed_scores_dsqn_quantized_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smoothed_scores_dsqn_quantized = [smoothed_scores_dsqn_quantized_all[best_runs[0]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[1]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[2]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[3]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[4]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[5]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[6]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[7]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[8]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[9]]]\n",
    "mean_smoothed_scores_dsqn_quantized = np.mean(best_smoothed_scores_dsqn_quantized, axis=0)\n",
    "\n",
    "avg_dsqn_quantized_completion_after = np.mean([dsqn_quantized_completion_after[best_runs[0]],\n",
    "                                dsqn_quantized_completion_after[best_runs[1]],\n",
    "                                dsqn_quantized_completion_after[best_runs[2]],\n",
    "                                dsqn_quantized_completion_after[best_runs[3]],\n",
    "                                dsqn_quantized_completion_after[best_runs[4]],\n",
    "                                dsqn_quantized_completion_after[best_runs[5]],\n",
    "                                dsqn_quantized_completion_after[best_runs[6]],\n",
    "                                dsqn_quantized_completion_after[best_runs[7]],\n",
    "                                dsqn_quantized_completion_after[best_runs[8]],\n",
    "                                dsqn_quantized_completion_after[best_runs[9]]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dsqn_quantized[0])), mean_smoothed_scores_dsqn_quantized)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn_quantized[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn_quantized, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn_quantized, 97, axis=0), alpha=0.25)\n",
    "\n",
    "plt.vlines(avg_dsqn_quantized_completion_after, 0, 250, 'C0')\n",
    "\n",
    "\n",
    "plt.ylim(0, 250)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DSQN_training.png', dpi=1000)\n",
    "plt.title('CartPole-v0 DSQN Quantized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot smoothed DQN vs. DSQN Training\n",
    "#mean_smoothed_scores_dqn = np.mean(smoothed_scores_dqn_all, axis=0)\n",
    "#mean_smoothed_scores_dsqn = np.mean(smoothed_scores_dsqn_all, axis=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "dqn = plt.plot(range(len(best_smoothed_scores_dqn[0])), mean_smoothed_scores_dqn, color='C0', label='DQN')\n",
    "plt.fill_between(range(len(best_smoothed_scores_dqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 97, axis=0), alpha=0.25)\n",
    "plt.vlines(avg_dqn_completion_after, 0, 250, 'C0')\n",
    "\n",
    "dsqn = plt.plot(range(len(best_smoothed_scores_dsqn[0])), mean_smoothed_scores_dsqn, color='C1', label='DSQN')\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 97, axis=0), alpha=0.25)\n",
    "plt.vlines(avg_dsqn_completion_after, 0, 250, 'C1')\n",
    "\n",
    "dsqn_quantized = plt.plot(range(len(best_smoothed_scores_dsqn_quantized[0])), mean_smoothed_scores_dsqn_quantized, color='C2', label='Quantized DSQN')\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn_quantized[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn_quantized, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn_quantized, 97, axis=0), alpha=0.25)\n",
    "plt.vlines(avg_dsqn_quantized_completion_after, 0, 250, 'C2')\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 250)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('sum of rewards')\n",
    "plt.title(env_name)\n",
    "plt.savefig(result_dir + '/DQN_vs_DSQN_training.png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate trained DQN and DSQN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_evaluation_seeds = [random.getrandbits(32) for _ in range(n_evaluations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test best trained DQN on the same environment for 200 timesteps\n",
    "evaluation_dqn_200 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dqn = QNetwork(architecture, 1).to(device)\n",
    "    dqn.load_state_dict(torch.load(result_dir + '/checkpoint_DQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dqn, 100, 200, gym_evaluation_seeds)\n",
    "    evaluation_dqn_200.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dqn_200', evaluation_dqn_200)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dqn_200)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dqn_200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best trained DQN on the same environment for 500 timesteps\n",
    "evaluation_dqn_500 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dqn = QNetwork(architecture, 1).to(device)\n",
    "    dqn.load_state_dict(torch.load(result_dir + '/checkpoint_DQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dqn, 100, 500, gym_evaluation_seeds)\n",
    "    evaluation_dqn_500.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dqn_500', evaluation_dqn_500)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dqn_500)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dqn_500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best trained DQN on the same environment for 1000 timesteps\n",
    "evaluation_dqn_1000 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dqn = QNetwork(architecture, 1).to(device)\n",
    "    dqn.load_state_dict(torch.load(result_dir + '/checkpoint_DQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dqn, 100, 1000, gym_evaluation_seeds)\n",
    "    evaluation_dqn_1000.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dqn_1000', evaluation_dqn_1000)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dqn_1000)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dqn_1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best trained DSQN on the same environment for 200 timesteps\n",
    "evaluation_dsqn_200 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "    dsqn.load_state_dict(torch.load(result_dir + '/checkpoint_DSQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dsqn, 100, 200, gym_evaluation_seeds)\n",
    "    evaluation_dsqn_200.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dsqn_200', evaluation_dsqn_200)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dsqn_200)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dsqn_200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best trained DSQN on the same environment for 200 timesteps\n",
    "evaluation_dsqn_500 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "    dsqn.load_state_dict(torch.load(result_dir + '/checkpoint_DSQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dsqn, 100, 500, gym_evaluation_seeds)\n",
    "    evaluation_dsqn_500.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dsqn_500', evaluation_dsqn_500)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dsqn_500)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dsqn_500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best trained DSQN on the same environment for 200 timesteps\n",
    "evaluation_dsqn_1000 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "    dsqn.load_state_dict(torch.load(result_dir + '/checkpoint_DSQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dsqn, 100, 1000, gym_evaluation_seeds)\n",
    "    evaluation_dsqn_1000.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dsqn_1000', evaluation_dsqn_1000)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dsqn_1000)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dsqn_1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [np.mean(evaluation_dqn_200), np.mean(evaluation_dsqn_200)]\n",
    "stds = [np.std(evaluation_dqn_200), np.std(evaluation_dsqn_200)]\n",
    "#x_pos = np.arange(len(means))\n",
    "x_pos = [0.5, .65]\n",
    "\n",
    "plt.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, capsize=10, width=0.1)\n",
    "plt.ylim(0, 250)\n",
    "plt.xticks(x_pos, ['DQN', 'DSQN'])\n",
    "plt.ylabel('Accumlative Reward')\n",
    "plt.title('CartPole-v0 Evaluation over 200 timesteps')\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/CartPole_evaluation_200.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [np.mean(evaluation_dqn_500), np.mean(evaluation_dsqn_500)]\n",
    "stds = [np.std(evaluation_dqn_500), np.std(evaluation_dsqn_500)]\n",
    "x_pos = [0.5, .65]\n",
    "\n",
    "plt.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, capsize=10, width=0.1)\n",
    "plt.ylim(0, 550)\n",
    "plt.xticks(x_pos, ['DQN', 'DSQN'])\n",
    "plt.ylabel('Accumlative Reward')\n",
    "plt.title('CartPole-v0 Evaluation over 500 timesteps')\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/CartPole_evaluation_500.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [np.mean(evaluation_dqn_1000), np.mean(evaluation_dsqn_1000)]\n",
    "stds = [np.std(evaluation_dqn_1000), np.std(evaluation_dsqn_1000)]\n",
    "x_pos = [0.5, .65]\n",
    "\n",
    "plt.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, capsize=10, width=0.1)\n",
    "plt.ylim(0, 1150)\n",
    "plt.xticks(x_pos, ['DQN', 'DSQN'])\n",
    "plt.ylabel('Accumlative Reward')\n",
    "plt.title('CartPole-v0 Evaluation over 1000 timesteps')\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/CartPole_evaluation_1000.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the membrane potential of the first layer, first item in batch\n",
    "potential = [mem[1][0] for mem in mem_rec]\n",
    "neuron1 = [p[0] for p in potential]\n",
    "neuron2 = [p[1] for p in potential]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the membrane potential for both output neurons for one random run before training\n",
    "plt.plot(neuron1, color='b', label='Output Neuron 1')\n",
    "plt.plot(neuron2, color='g', label='Output Neuron 2')\n",
    "plt.grid(True)\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel('time steps')\n",
    "plt.ylabel('membrane potential')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('cartpole_output_neurons_potential_b4_training.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the membrane potential of the hidden layer neurons\n",
    "potential = [mem[0][0] for mem in mem_rec]\n",
    "neurons = []\n",
    "for i in range(len(potential[0])):\n",
    "    neurons.append([p[i] for p in potential])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the membrane potential for the hidden layer neurons\n",
    "for i in range(len(neurons)):\n",
    "    plt.plot(neurons[i], label='neuron {}'.format(i + 1))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('membrane potential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the play buffer with some data\n",
    "env = gym.make(env_name)\n",
    "memory = ReplayBuffer(replay_memory_size, batch_size, random_seeds[0])\n",
    "for i in range(1000):\n",
    "    print(\"Episode: {}\".format(i), end='\\r')\n",
    "    state = env.reset()\n",
    "    for t in range(1000):\n",
    "        action = random.randint(0, 1)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sunblaze_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_env = sunblaze_envs.make('SunblazeCartPoleRandomNormal-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'result_20_2021122'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dsqn_random_200 = []\n",
    "\n",
    "dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "optimizer = optim.Adam(dsqn.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "    dsqn.load_state_dict(torch.load(result_dir + '/checkpoint_DSQN_{}.pt'.format(i)))\n",
    "    \n",
    "    agent = Agent(env_name, dsqn, dsqn, architecture, batch_size,\n",
    "              replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "              update_every, target_update_frequency, optimizer, learning_rate,\n",
    "              num_episodes, max_steps, 0, result_dir, 0, tau, SQN=True, two_neurons=False, random=True)\n",
    "    \n",
    "    rewards = agent.evaluate_agent(dsqn, 100, 200, gym_evaluation_seeds)\n",
    "    evaluation_dsqn_random_200.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dsqn_200', evaluation_dsqn_random_200)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dsqn_random_200)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dsqn_random_200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dqn_random_200 = []\n",
    "\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dqn = QNetwork(architecture, 1).to(device)\n",
    "    dqn.load_state_dict(torch.load(result_dir + '/checkpoint_DQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dqn, 100, 200, gym_evaluation_seeds)\n",
    "    evaluation_dqn_random_200.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dqn_200', evaluation_dqn_random_200)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dqn_random_200)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dqn_random_200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
