{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "training_q_learning.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "ZFW05dxqY7nU",
    "9m4PEscghHug",
    "uHRj7pVIY7nb",
    "IvqltimhY7nh",
    "fT8o7oXAY7ni"
   ]
  },
  "kernelspec": {
   "name": "dsqnvenv",
   "language": "python",
   "display_name": "DSQNvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuWLQT-1Y7nC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DQN vs. DSQN for the CartPole Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oipXcfdsJ-L",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ewRHcGuKgQkq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "basis_dir = './results/'"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#@title Imports{ form-width: \"20%\", display-mode: \"form\" }\n",
    "import os\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "from agent import Agent, ReplayBuffer\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textwrap import wrap\n",
    "\n",
    "\n",
    "#%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported default model\n"
     ]
    }
   ],
   "source": [
    "experiment_type = [\"default\",\"twoneuron\", \"ttfs\",\"poisson\" , \"fre\"]\n",
    "type_nr = 0\n",
    "\n",
    "from model import QNetwork, DSNN\n",
    "print(\"Imported {} model\".format(experiment_type[type_nr]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Environment specific parameters\n",
    "env_name = 'CartPole-v0'\n",
    "n_runs = 5\n",
    "n_evaluations = 100\n",
    "max_steps = 200\n",
    "num_episodes = 1000\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['result_32_ttfs_2022622', 'result_12_poisson_2022324', 'result_8_poisson_2022322_pop', 'result_11_poisson_2022322', 'result_21_fre_2022411', 'result_25_poisson_2022426', 'result_14_poisson_2022328', 'result_9_poisson_2022124', 'result_10_poisson_2022322', 'result_1_poisson_20211215', 'result_13_fre_2022324', 'result_11_poisson_2022125', 'decent_ttfs_2', 'result_31_ttfs_2022518', 'result_21_poisson_2022412', 'result_10_poisson_2022124', 'result_15_poisson_2022329', 'result_24_poisson_2022412', 'decent_ttfs_5', 'result_29_poisson_2022514', 'result_20_poisson_2022411', 'result_18_fre_2022411', 'result_30_ttfs_2022516', 'result_7_poisson_202231', 'result_30_ttfs_2022515', 'result_23_poisson_2022412', 'result_9_poisson_2022322', 'result_4_poisson_202213', 'result_27_poisson_2022427', 'result_28_poisson_2022514', 'result_6_ttfs_2022119', 'result_16_poisson_2022329', 'result_17_fre_202247', 'result_22_poisson_2022412']\n",
      "Created Directory ./results/result_33_default_202272 to store the results in\n"
     ]
    }
   ],
   "source": [
    "# Create Results Directory\n",
    "\n",
    "dirs = os.listdir(basis_dir)\n",
    "print(dirs)\n",
    "if not any('result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = basis_dir  +'result_'+ str(result_id) + '_' + experiment_type[type_nr] + '_{}'.format(\n",
    "    str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#@title Change Result Directory { form-width: \"20%\", display-mode: \"form\" }\n",
    "#result_dir = 'result_12_20211028'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#@title Hyperparameters { form-width: \"20%\", display-mode: \"form\" }\n",
    "batch_size = 1\n",
    "discount_factor = 0.999\n",
    "eps_start = 1.0\n",
    "eps_end = 0.05\n",
    "eps_decay = 0.999\n",
    "update_every = 4\n",
    "target_update_frequency = 100\n",
    "learning_rate = 0.003\n",
    "replay_memory_size = 4*10**4\n",
    "tau = 1e-3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#@title SNN Hyperparameters { form-width: \"20%\", display-mode: \"form\" }\n",
    "time_step = 1e-3 # 1ms\n",
    "simulation_time = 10\n",
    "weight_scale = 1\n",
    "threshold = 0.1\n",
    "architecture = [4, 64, 64, 2]\n",
    "\n",
    "# Tau\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "alpha = 0.1\n",
    "beta = 0.8\n",
    "\n",
    "#Calculated values, as can be found in XXXXXX\n",
    "#alpha   = float(np.exp(-time_step/tau_syn))\n",
    "#beta    = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "two_neuron=False\n",
    "population_coding=False\n",
    "population_size = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "seeds = [random.getrandbits(32) for _ in range(n_runs)]\n",
    "np.save(result_dir + '/' + 'seeds', seeds)\n",
    "#seeds = np.load(basis_dir + 'seeds.npy').tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#@title Useful Functions { form-width: \"20%\", display-mode: \"form\" }\n",
    "def loadScores(score_dir, score_name, amount):\n",
    "    score_list = []\n",
    "    for i in range(amount):\n",
    "      score_list.append(np.load(score_dir + '/' + score_name + '_{}'.format(i) + '.npy'))\n",
    "    return score_list\n",
    "\n",
    "def create_params_dict():\n",
    "    params = {}\n",
    "    params.update( {'alpha' : alpha} )\n",
    "    params.update( {'beta' : beta} )\n",
    "    params.update( {'threshold' : threshold} )\n",
    "    params.update( {'batch_size' : batch_size} )\n",
    "    params.update( {'discount_factor' : discount_factor} )\n",
    "    params.update( {'eps_start' : eps_start} )\n",
    "    params.update( {'eps_end' : eps_end} )\n",
    "    params.update( {'eps_decay' : eps_decay} )\n",
    "    params.update( {'update_every' : update_every} )\n",
    "    params.update( {'target_update_frequency' : target_update_frequency} )\n",
    "    params.update( {'learning_rate' : learning_rate} )\n",
    "    params.update( {'replay_memory_size' : replay_memory_size} )\n",
    "\n",
    "    params.update( {'tau' : tau} )\n",
    "    params.update( {'time_step' : time_step} )\n",
    "    params.update( {'simulation_time' : simulation_time} )\n",
    "\n",
    "    params.update( {'weight_scale' : weight_scale} )\n",
    "    params.update( {'architecture' : architecture} )\n",
    "    params.update( {'seeds' : seeds} )\n",
    "\n",
    "    params.update( {'population_coding' : population_coding} )\n",
    "    params.update( {'population_size' : population_size} )\n",
    "    params.update( {'two_neuron' : two_neuron} )\n",
    "    return params\n",
    "\n",
    "def saveHyperparametersToFile():\n",
    "    params = create_params_dict()\n",
    "\n",
    "    # Serialize data into file:\n",
    "    json.dump( params, open( result_dir +\"/hyperparameters.json\", 'w' ) )\n",
    "\n",
    "def getDateTime():\n",
    "  return datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")  \n",
    "\n",
    "# Plot scores of individual runs\n",
    "def plot_score(smoothed_score,i, params = None):\n",
    "    plt.clf()\n",
    "    if params:\n",
    "        param_title = str(params)\n",
    "        plt.title('\\n'.join(wrap(param_title,60)), fontsize=8)\n",
    "    plt.plot(smoothed_score)\n",
    "    plt.ylim(0, 250)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(result_dir + '/training_dsqn_{}.png'.format(i), dpi=1000)\n",
    "    #plt.show()\n",
    "\n",
    "def init_scaler(two_neuron = False):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    if two_neuron:\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "    if env_name == 'MountainCar-v0':\n",
    "        limits = np.asarray([[-1.2, -0.07],\n",
    "                          [0.6, 0.07]])\n",
    "    else:\n",
    "        #[position of cart, velocity of cart, angle of pole, rotation rate of pole]\n",
    "        limits = np.asarray([[-4.8, -3, -0.21 , -3],\n",
    "                          [4.8, 3 , 0.21, 3]])\n",
    "\n",
    "    # fit data\n",
    "    scaler.fit(limits)\n",
    "    return scaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DSQN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DSQN Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Enables the python console on exeption\n",
    "%pdb off"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type_nr = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Reload changed model file { form-width: \"20%\", display-mode: \"form\" }\n",
    "model_string = \"default\"\n",
    "if experiment_type[type_nr] == \"twoneuron\":\n",
    "    model_string = \"twoneuron\"\n",
    "    import model_twoneurons\n",
    "    importlib.reload(model_twoneurons)\n",
    "    from model_twoneurons import QNetwork, DSNN\n",
    "elif experiment_type[type_nr] == \"poisson\":\n",
    "    model_string = \"poisson\"\n",
    "    import model_poisson\n",
    "    importlib.reload(model_poisson)\n",
    "    from model_poisson import QNetwork, DSNN\n",
    "elif experiment_type[type_nr] == \"ttfs\":\n",
    "    model_string = \"ttfs\"\n",
    "    import model_ttfs\n",
    "    importlib.reload(model_ttfs)\n",
    "    from model_ttfs import QNetwork, DSNN\n",
    "elif experiment_type[type_nr] == \"fre\":\n",
    "    model_string = \"fre\"\n",
    "    import model_fre\n",
    "    importlib.reload(model_fre)\n",
    "    from model_fre import QNetwork, DSNN\n",
    "else :\n",
    "    import model\n",
    "    importlib.reload(model)\n",
    "    from model import QNetwork, DSNN\n",
    "print(\"Reloaded {} model\".format(model_string))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Reload changed agent file { form-width: \"20%\", display-mode: \"form\" }\n",
    "\n",
    "import agent\n",
    "importlib.reload(agent)\n",
    "from agent import Agent, ReplayBuffer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import agent_tr\n",
    "importlib.reload(agent_tr)\n",
    "from agent_tr import Agent, ReplayBuffer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DSQN Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DSQNs with two-neurons-input encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gridsearch Approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "architecture = [4, 64, 64, 2]\n",
    "possible_threshold_boundaries = [0.3,0.3]\n",
    "possible_alpha_boundaries = [0.1, 0.1]\n",
    "possible_beta_boundaries = [0.8, 0.8]\n",
    "\n",
    "parameter_permutations = []\n",
    "granularity = 20\n",
    "\n",
    "possible_alphas = np.linspace(possible_alpha_boundaries[0],possible_alpha_boundaries[1],int(granularity/4))\n",
    "possible_betas = np.linspace(possible_beta_boundaries[0],possible_beta_boundaries[1],int(granularity/4))\n",
    "possible_thresholds = np.linspace(possible_threshold_boundaries[0],possible_threshold_boundaries[1],granularity)\n",
    "\n",
    "for a in possible_alphas:\n",
    "    for b in possible_betas:\n",
    "        for t in possible_thresholds:\n",
    "            parameter_permutations.append([a,b,t])\n",
    "\n",
    "scaler = init_scaler(two_neuron)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def create3DPlot(permutations, current_alpha, train_res, use_alphas=False):\n",
    "\n",
    "    tmp_split = np.array(permutations.get(str(current_alpha)), dtype=object).reshape(-1,2)\n",
    "\n",
    "    results_for_alpha = tmp_split[:,1]\n",
    "\n",
    "    #permuts_for_alpha = np.array(np.array(tmp_split[:,0]).tolist())\n",
    "    #betas = permuts_for_alpha[:,0]\n",
    "    #thresholds = permuts_for_alpha[:,1]\n",
    "\n",
    "    #threshold dimensions\n",
    "    dim2 = len(possible_thresholds)\n",
    "\n",
    "    dim1 = len(possible_betas)\n",
    "\n",
    "    max_number_values = dim1*dim2\n",
    "\n",
    "    def empty_grid():\n",
    "        coord_pairs = []\n",
    "        for b in possible_betas:\n",
    "            for t in possible_thresholds:\n",
    "                coord_pairs.append([b,t])\n",
    "        return np.array(coord_pairs)\n",
    "\n",
    "    empty = empty_grid()\n",
    "    X = empty[:,0].reshape((dim1,dim2))\n",
    "    Y = empty[:,1].reshape((dim1,dim2))\n",
    "\n",
    "    new_train_res = np.hstack((results_for_alpha,[-1]*(max_number_values - len(results_for_alpha))))\n",
    "    #Z = np.reshape(np.ma.array(new_train_res, mask=([0]*(len(train_res))+[1]*(len(new_train_res)-len(train_res)))), (dim1,dim2))\n",
    "    Z = new_train_res.reshape((dim1,dim2))\n",
    "\n",
    "    # Plot the surface.\n",
    "    plt.clf()\n",
    "    plt.ioff()\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                         linewidth=0, antialiased=False)\n",
    "\n",
    "    # Customize the z axis.\n",
    "    ax.set_xlim( np.min(possible_betas), np.max(possible_betas))\n",
    "    ax.set_ylim( np.min(possible_thresholds), np.max(possible_thresholds))\n",
    "    ax.set_zlim( -1, 205)\n",
    "    #ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "\n",
    "    ax.set_xlabel('Beta')\n",
    "    ax.set_ylabel('Threshold')\n",
    "    ax.set_zlabel('#Steps')\n",
    "    plt.rcParams['axes.grid'] = False\n",
    "    plt.figtext(0.5, 0.01, f'Plot for alpha {current_alpha}', ha=\"center\")\n",
    "\n",
    "    # Add a color bar which maps values to colors.\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "    plt.savefig(result_dir + f'/3D_plot_a{current_alpha}.png', dpi=1000)\n",
    "    plt.close(fig)\n",
    "\n",
    "#create3DPlot(set_permutations, alpha, res_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import model\n",
    "importlib.reload(model)\n",
    "from model import QNetwork, DSNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run # 0 at 02/07/2022 17:59:34\n",
      "Episode 100\tAverage Score: 14.29\t Epsilon: 0.24\n",
      "Episode 200\tAverage Score: 10.99\t Epsilon: 0.08\n",
      "Episode 300\tAverage Score: 14.54\t Epsilon: 0.05\n",
      "Episode 400\tAverage Score: 16.79\t Epsilon: 0.05\n",
      "Episode 451\tAverage Score: 15.22\t Epsilon: 0.05\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_2294487/655120111.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     35\u001B[0m                   num_episodes, max_steps, i_run, result_dir, seed, tau, SQN=True, quantization=False)\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m     \u001B[0msmoothed_scores\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbest_average_after\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbest_average\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_agent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult_dir\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'/scores_{}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi_run\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscores\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Private/Uni/MA/Code/dsqn_openaigym/DQN-Cartpole/agent.py\u001B[0m in \u001B[0;36mtrain_agent\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    280\u001B[0m                 \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m                 \u001B[0mnext_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 282\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnext_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    283\u001B[0m                 \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext_state\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m                 \u001B[0mscore\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Private/Uni/MA/Code/dsqn_openaigym/DQN-Cartpole/agent.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, state, action, reward, next_state, done, loss_hist)\u001B[0m\n\u001B[1;32m    195\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmemory\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    196\u001B[0m                 \u001B[0mexperiences\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmemory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 197\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimize_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexperiences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_hist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m         \u001B[0;31m# For training purposes, create sample of reasonable environment states\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Private/Uni/MA/Code/dsqn_openaigym/DQN-Cartpole/agent.py\u001B[0m in \u001B[0;36moptimize_model\u001B[0;34m(self, experiences, loss_hist)\u001B[0m\n\u001B[1;32m    210\u001B[0m         \u001B[0;31m# Get max predicted Q values (for next states) from target model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    211\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSQN\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 212\u001B[0;31m             \u001B[0mQ_targets_next\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtarget_net\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnext_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    213\u001B[0m                 \u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    214\u001B[0m                 \u001B[0;31m# gather(1, actions)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Private/Uni/MA/Code/dsqn_openaigym/DQN-Cartpole/model.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inputs, loihi)\u001B[0m\n\u001B[1;32m    190\u001B[0m                         \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"ab,bc->ac\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    191\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 192\u001B[0;31m                         \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"ab,bc->ac\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mspk_rec\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    193\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0;31m# calculate the new synapse potential\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Private/Uni/MA/Code/dsqn_openaigym/DQN-Cartpole/DSQNvenv/lib/python3.8/site-packages/torch/functional.py\u001B[0m in \u001B[0;36meinsum\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    323\u001B[0m         \u001B[0;31m# recurse incase operands contains value that has torch function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    324\u001B[0m         \u001B[0;31m# in the original implementation this line is omitted\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 325\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0meinsum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mequation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0m_operands\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    327\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0m_VF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mequation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moperands\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[attr-defined]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Private/Uni/MA/Code/dsqn_openaigym/DQN-Cartpole/DSQNvenv/lib/python3.8/site-packages/torch/functional.py\u001B[0m in \u001B[0;36meinsum\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    325\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0meinsum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mequation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0m_operands\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    326\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 327\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_VF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meinsum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mequation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moperands\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[attr-defined]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    329\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "smoothed_scores_dsqn_all = []\n",
    "dsqn_completion_after = []\n",
    "\n",
    "res_list= []\n",
    "set_permutations = {}\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "\n",
    "for i_run, cur_permutation in enumerate(parameter_permutations[start_index:]):\n",
    "    print(\"Run # {}\".format(i_run) + ' at '+getDateTime())\n",
    "    seed = seeds[i_run%n_runs]\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # current hyperparameters\n",
    "    alpha = cur_permutation[0]\n",
    "    beta = cur_permutation[1]\n",
    "    threshold = cur_permutation[2]\n",
    "    params = create_params_dict()\n",
    "\n",
    "    #policy_net = DSNN(architecture, seed, alpha, beta, batch_size, threshold, simulation_time, scaler, two_neuron=two_neuron , population_coding=population_coding, population_size=population_size )\n",
    "    #target_net = DSNN(architecture, seed, alpha, beta, batch_size, threshold, simulation_time, scaler, two_neuron=two_neuron , population_coding=population_coding, population_size=population_size )\n",
    "\n",
    "    policy_net = DSNN(architecture, seed, alpha, beta, batch_size, threshold, simulation_time,  scaler=None, two_neuron=two_neuron , population_coding=population_coding, population_size=population_size, add_bias = False, encoding=experiment_type[type_nr], decoding=\"potential\")\n",
    "    target_net = DSNN(architecture, seed, alpha, beta, batch_size, threshold, simulation_time, scaler=None, two_neuron=two_neuron , population_coding=population_coding, population_size=population_size, add_bias = False, encoding=experiment_type[type_nr], decoding=\"potential\")\n",
    "\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau, SQN=True, quantization=False)\n",
    "\n",
    "    smoothed_scores, scores, best_average_after, best_average = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DSQN_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    plot_score(smoothed_scores,i_run, params)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dsqn_all.append(smoothed_scores)\n",
    "    dsqn_completion_after.append(best_average_after)\n",
    "\n",
    "    res_list.append(best_average)\n",
    "\n",
    "    np.savez(result_dir + '/best_res', res_list, parameter_permutations)\n",
    "\n",
    "    set_permutations.setdefault(str(alpha), ([])).append([cur_permutation[1:3], best_average])\n",
    "\n",
    "    create3DPlot(set_permutations, alpha, res_list)\n",
    "\n",
    "    print(\"\")\n",
    "print(\"Finished at \"+ getDateTime())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "saveHyperparametersToFile()\n",
    "\n",
    "smoothed_scores_dsqn_all = []\n",
    "dsqn_completion_after = []\n",
    "simulation_time = 10\n",
    "scaler = init_scaler()\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run) + ' at '+ getDateTime())\n",
    "    seed = seeds[i_run]\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    policy_net = DSNN(architecture, seed, alpha, beta, 1, threshold, simulation_time, scaler)\n",
    "    target_net = DSNN(architecture, seed, alpha, beta, 1, threshold, simulation_time, scaler)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau, SQN=True, quantization=False)\n",
    "\n",
    "    smoothed_scores, scores, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DSQN_{}'.format(i_run), smoothed_scores)\n",
    "    plot_score(smoothed_scores,i_run)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dsqn_all.append(smoothed_scores)\n",
    "    dsqn_completion_after.append(best_average_after)\n",
    "    print(\"\")\n",
    "print(\"Finished at \"+ getDateTime())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DSQN Plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_dir = \"/content/drive/My Drive/Uni/MasterArbeit/dsqn_examples/results/result_20_default_2021113\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smoothed_scores_dsqn_all = loadScores(result_dir,\"smoothed_scores_DSQN\",5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot scores of individual runs\n",
    "for i in range(len(smoothed_scores_dsqn_all)):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(smoothed_scores_dsqn_all[i])\n",
    "    plt.ylim(0, 250)\n",
    "    plt.grid(True)\n",
    "    #plt.savefig(result_dir + '/training_dsqn_{}.png'.format(i), dpi=1000)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_runs = [ i for i in range(29)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(smoothed_scores_dsqn_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_smoothed_scores_dsqn = [[]]*len(smoothed_scores_dsqn_all)\n",
    "for i in range(len(smoothed_scores_dsqn_all)):\n",
    "    best_smoothed_scores_dsqn[i] = smoothed_scores_dsqn_all[best_runs[i]]\n",
    "\n",
    "mean_smoothed_scores_dsqn = np.mean(best_smoothed_scores_dsqn, axis=0)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dsqn[0])), mean_smoothed_scores_dsqn)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 97, axis=0), alpha=0.25)\n",
    "\n",
    "try: # in case the notebook expires, the dsqn_completions cannot be reloaed\n",
    "    if(dsqn_completion_after):\n",
    "      avg_dsqn_completion_after = [[]]*len(dsqn_completion_after)\n",
    "      for i in range(len(dsqn_completion_after)):\n",
    "          avg_dsqn_completion_after[i] = dsqn_completion_after[best_runs[i]]\n",
    "      avg_dsqn_completion_after = np.mean(avg_dsqn_completion_after)\n",
    "      plt.vlines(avg_dsqn_completion_after, 0, 250, 'C0')\n",
    "except NameError:\n",
    "    dsqn_completion_after = None\n",
    "\n",
    "\n",
    "plt.ylim(0, 250)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DSQN_training.png', dpi=1000)\n",
    "plt.title('CartPole-v0 DSQN')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quantized DSQN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantized DSQN Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smoothed_scores_dsqn_quantized_all = []\n",
    "dsqn_quantized_completion_after = []\n",
    "simulation_time = 8\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run))\n",
    "    seed = seeds[i_run]\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    policy_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time)\n",
    "    target_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau, SQN=True, two_neurons=False,\n",
    "                  quantization=True)\n",
    "\n",
    "    smoothed_scores, scores, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DSQN_Loihi_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dsqn_quantized_all.append(smoothed_scores)\n",
    "    dsqn_quantized_completion_after.append(best_average_after)\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smoothed_scores_dsqn_quantized_all = smoothed_scores_dsqn_all\n",
    "dsqn_quantized_completion_after = dsqn_completion_after"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time, two_neurons=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dsqn_completion_after"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy_net.weights = weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = seeds[0]\n",
    "policy_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time, two_neurons=False)\n",
    "target_net = DSQN(architecture, seed, alpha, beta, weight_scale, batch_size, threshold, simulation_time, two_neurons=False)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, 0, result_dir, seed, tau, SQN=True, two_neurons=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weights = policy_net.weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q_weights = agent.quantize_weights(weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quant_weights = [q_w.tensor.float() for q_w in q_weights]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quant_weights[0].requires_grad = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quant_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "step = (1.8 + 1.8)/255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = np.concatenate((weights[0].detach().numpy()[0], weights[0].detach().numpy()[1], weights[0].detach().numpy()[2], weights[0].detach().numpy()[3]))\n",
    "bins = np.arange(-1.8, 1.8, step)\n",
    "plt.hist(w, bins)\n",
    "plt.title('FP32 Weights')\n",
    "plt.savefig('weights_fp32.png', dpi=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = np.concatenate((quant_weights[0].detach().numpy()[0], quant_weights[0].detach().numpy()[1], quant_weights[0].detach().numpy()[2], quant_weights[0].detach().numpy()[3]), axis=0)\n",
    "bins = range(-128, 127)\n",
    "plt.hist(w, bins)\n",
    "plt.title('Quantized Weights')\n",
    "plt.savefig('weights_quantized.png', dpi=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy_net.weights = quant_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot Quantized DSQN Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smoothed_scores_dsqn_quantized_0 = np.load('result_23_2021416/smoothed_scores_DSQN_0.npy')\n",
    "smoothed_scores_dsqn_quantized_1 = np.load('result_23_2021416/smoothed_scores_DSQN_1.npy')\n",
    "smoothed_scores_dsqn_quantized_2 = np.load('result_23_2021416/smoothed_scores_DSQN_2.npy')\n",
    "smoothed_scores_dsqn_quantized_3 = np.load('result_23_2021416/smoothed_scores_DSQN_3.npy')\n",
    "smoothed_scores_dsqn_quantized_4 = np.load('result_23_2021416/smoothed_scores_DSQN_4.npy')\n",
    "smoothed_scores_dsqn_quantized_5 = np.load('result_23_2021416/smoothed_scores_DSQN_5.npy')\n",
    "smoothed_scores_dsqn_quantized_6 = np.load('result_23_2021416/smoothed_scores_DSQN_6.npy')\n",
    "smoothed_scores_dsqn_quantized_7 = np.load('result_23_2021416/smoothed_scores_DSQN_7.npy')\n",
    "smoothed_scores_dsqn_quantized_8 = np.load('result_23_2021416/smoothed_scores_DSQN_8.npy')\n",
    "smoothed_scores_dsqn_quantized_9 = np.load('result_23_2021416/smoothed_scores_DSQN_9.npy')\n",
    "smoothed_scores_dsqn_quantized_all = [smoothed_scores_dsqn_quantized_0, smoothed_scores_dsqn_quantized_1, smoothed_scores_dsqn_quantized_2, smoothed_scores_dsqn_quantized_3, smoothed_scores_dsqn_quantized_4, smoothed_scores_dsqn_quantized_5, smoothed_scores_dsqn_quantized_6, smoothed_scores_dsqn_quantized_7, smoothed_scores_dsqn_quantized_8, smoothed_scores_dsqn_quantized_9]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_smoothed_scores_dsqn_quantized = [smoothed_scores_dsqn_quantized_all[best_runs[0]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[1]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[2]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[3]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[4]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[5]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[6]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[7]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[8]],\n",
    "                             smoothed_scores_dsqn_quantized_all[best_runs[9]]]\n",
    "mean_smoothed_scores_dsqn_quantized = np.mean(best_smoothed_scores_dsqn_quantized, axis=0)\n",
    "\n",
    "avg_dsqn_quantized_completion_after = np.mean([dsqn_quantized_completion_after[best_runs[0]],\n",
    "                                dsqn_quantized_completion_after[best_runs[1]],\n",
    "                                dsqn_quantized_completion_after[best_runs[2]],\n",
    "                                dsqn_quantized_completion_after[best_runs[3]],\n",
    "                                dsqn_quantized_completion_after[best_runs[4]],\n",
    "                                dsqn_quantized_completion_after[best_runs[5]],\n",
    "                                dsqn_quantized_completion_after[best_runs[6]],\n",
    "                                dsqn_quantized_completion_after[best_runs[7]],\n",
    "                                dsqn_quantized_completion_after[best_runs[8]],\n",
    "                                dsqn_quantized_completion_after[best_runs[9]]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dsqn_quantized[0])), mean_smoothed_scores_dsqn_quantized)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn_quantized[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn_quantized, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn_quantized, 97, axis=0), alpha=0.25)\n",
    "\n",
    "plt.vlines(avg_dsqn_quantized_completion_after, 0, 250, 'C0')\n",
    "\n",
    "\n",
    "plt.ylim(0, 250)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DSQN_training.png', dpi=1000)\n",
    "plt.title('CartPole-v0 DSQN Quantized')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot smoothed DQN vs. DSQN Training\n",
    "#mean_smoothed_scores_dqn = np.mean(smoothed_scores_dqn_all, axis=0)\n",
    "#mean_smoothed_scores_dsqn = np.mean(smoothed_scores_dsqn_all, axis=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "dqn = plt.plot(range(len(best_smoothed_scores_dqn[0])), mean_smoothed_scores_dqn, color='C0', label='DQN')\n",
    "plt.fill_between(range(len(best_smoothed_scores_dqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 97, axis=0), alpha=0.25)\n",
    "plt.vlines(avg_dqn_completion_after, 0, 250, 'C0')\n",
    "\n",
    "dsqn = plt.plot(range(len(best_smoothed_scores_dsqn[0])), mean_smoothed_scores_dsqn, color='C1', label='DSQN')\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 97, axis=0), alpha=0.25)\n",
    "plt.vlines(avg_dsqn_completion_after, 0, 250, 'C1')\n",
    "\n",
    "#dsqn_quantized = plt.plot(range(len(best_smoothed_scores_dsqn_quantized[0])), mean_smoothed_scores_dsqn_quantized, color='C2', label='Quantized DSQN')\n",
    "#plt.fill_between(range(len(best_smoothed_scores_dsqn_quantized[0])),\n",
    "#                 np.nanpercentile(best_smoothed_scores_dsqn_quantized, 2, axis=0),\n",
    "#                 np.nanpercentile(best_smoothed_scores_dsqn_quantized, 97, axis=0), alpha=0.25)\n",
    "#plt.vlines(avg_dsqn_quantized_completion_after, 0, 250, 'C2')\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 250)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('sum of rewards')\n",
    "plt.title(env_name)\n",
    "plt.savefig(result_dir + '/DQN_vs_DSQN_training.png', dpi=1000)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate trained DQN and DSQN models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gym_evaluation_seeds = [random.getrandbits(32) for _ in range(n_evaluations)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(agent)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from agent import Agent, ReplayBuffer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau, SQN=True, quantization=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test best trained DQN on the same environment for 200 timesteps\n",
    "evaluation_dqn_200 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dqn = QNetwork(architecture, 1).to(device)\n",
    "    dqn.load_state_dict(torch.load(result_dir + '/checkpoint_DQN_{}.pt'.format(i)))\n",
    "    rewards = test_agent.evaluate_agent(dqn, env, 100, 200, gym_evaluation_seeds, quantization=False)\n",
    "    evaluation_dqn_200.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dqn_200', evaluation_dqn_200)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dqn_200)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dqn_200)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test best trained DQN on the same environment for 500 timesteps\n",
    "evaluation_dqn_500 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dqn = QNetwork(architecture, 1).to(device)\n",
    "    dqn.load_state_dict(torch.load(result_dir + '/checkpoint_DQN_{}.pt'.format(i)))\n",
    "    rewards = test_agent.evaluate_agent(dqn, env, 100, 500, gym_evaluation_seeds, quantization=False)\n",
    "    evaluation_dqn_500.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dqn_500', evaluation_dqn_500)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dqn_500)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dqn_500)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test best trained DQN on the same environment for 1000 timesteps\n",
    "evaluation_dqn_1000 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dqn = QNetwork(architecture, 1).to(device)\n",
    "    dqn.load_state_dict(torch.load(result_dir + '/checkpoint_DQN_{}.pt'.format(i)))\n",
    "    rewards = test_agent.evaluate_agent(dqn, env, 100, 1000, gym_evaluation_seeds, quantization=False)\n",
    "    evaluation_dqn_1000.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dqn_1000', evaluation_dqn_1000)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dqn_1000)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dqn_1000)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test best trained DSQN on the same environment for 200 timesteps\n",
    "evaluation_dsqn_200 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "    dsqn.load_state_dict(torch.load(result_dir + '/checkpoint_DSQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dsqn, env, 100, 200, gym_evaluation_seeds, quantization=False)\n",
    "    evaluation_dsqn_200.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dsqn_200', evaluation_dsqn_200)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dsqn_200)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dsqn_200)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test best trained DSQN on the same environment for 200 timesteps\n",
    "evaluation_dsqn_500 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "    dsqn.load_state_dict(torch.load(result_dir + '/checkpoint_DSQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dsqn, 100, 500, gym_evaluation_seeds)\n",
    "    evaluation_dsqn_500.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dsqn_500', evaluation_dsqn_500)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dsqn_500)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dsqn_500)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test best trained DSQN on the same environment for 200 timesteps\n",
    "evaluation_dsqn_1000 = []\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "    dsqn.load_state_dict(torch.load(result_dir + '/checkpoint_DSQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dsqn, 100, 1000, gym_evaluation_seeds)\n",
    "    evaluation_dsqn_1000.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dsqn_1000', evaluation_dsqn_1000)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dsqn_1000)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dsqn_1000)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means = [np.mean(evaluation_dqn_200), np.mean(evaluation_dsqn_200)]\n",
    "stds = [np.std(evaluation_dqn_200), np.std(evaluation_dsqn_200)]\n",
    "#x_pos = np.arange(len(means))\n",
    "x_pos = [0.5, .65]\n",
    "\n",
    "plt.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, capsize=10, width=0.1)\n",
    "plt.ylim(0, 250)\n",
    "plt.xticks(x_pos, ['DQN', 'DSQN'])\n",
    "plt.ylabel('Accumlative Reward')\n",
    "plt.title('CartPole-v0 Evaluation over 200 timesteps')\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/CartPole_evaluation_200.png', dpi=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means = [np.mean(evaluation_dqn_500), np.mean(evaluation_dsqn_500)]\n",
    "stds = [np.std(evaluation_dqn_500), np.std(evaluation_dsqn_500)]\n",
    "x_pos = [0.5, .65]\n",
    "\n",
    "plt.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, capsize=10, width=0.1)\n",
    "plt.ylim(0, 550)\n",
    "plt.xticks(x_pos, ['DQN', 'DSQN'])\n",
    "plt.ylabel('Accumlative Reward')\n",
    "plt.title('CartPole-v0 Evaluation over 500 timesteps')\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/CartPole_evaluation_500.png', dpi=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means = [np.mean(evaluation_dqn_1000), np.mean(evaluation_dsqn_1000)]\n",
    "stds = [np.std(evaluation_dqn_1000), np.std(evaluation_dsqn_1000)]\n",
    "x_pos = [0.5, .65]\n",
    "\n",
    "plt.bar(x_pos, means, yerr=stds, align='center', alpha=0.5, capsize=10, width=0.1)\n",
    "plt.ylim(0, 1150)\n",
    "plt.xticks(x_pos, ['DQN', 'DSQN'])\n",
    "plt.ylabel('Accumlative Reward')\n",
    "plt.title('CartPole-v0 Evaluation over 1000 timesteps')\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/CartPole_evaluation_1000.png', dpi=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the membrane potential of the first layer, first item in batch\n",
    "potential = [mem[1][0] for mem in mem_rec]\n",
    "neuron1 = [p[0] for p in potential]\n",
    "neuron2 = [p[1] for p in potential]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the membrane potential for both output neurons for one random run before training\n",
    "plt.plot(neuron1, color='b', label='Output Neuron 1')\n",
    "plt.plot(neuron2, color='g', label='Output Neuron 2')\n",
    "plt.grid(True)\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel('time steps')\n",
    "plt.ylabel('membrane potential')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('cartpole_output_neurons_potential_b4_training.png', dpi=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the membrane potential of the hidden layer neurons\n",
    "potential = [mem[0][0] for mem in mem_rec]\n",
    "neurons = []\n",
    "for i in range(len(potential[0])):\n",
    "    neurons.append([p[i] for p in potential])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the membrane potential for the hidden layer neurons\n",
    "for i in range(len(neurons)):\n",
    "    plt.plot(neurons[i], label='neuron {}'.format(i + 1))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('membrane potential')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test Code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fill the play buffer with some data\n",
    "env = gym.make(env_name)\n",
    "memory = ReplayBuffer(replay_memory_size, batch_size, random_seeds[0])\n",
    "for i in range(1000):\n",
    "    print(\"Episode: {}\".format(i), end='\\r')\n",
    "    state = env.reset()\n",
    "    for t in range(1000):\n",
    "    for t in range(1000):\n",
    "        action = random.randint(0, 1)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qZTQhl8vY7no",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "random_env = sunblaze_envs.make('SunblazeCartPoleRandomNormal-v0')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wybf3NogY7no",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "result_dir = 'result_20_2021122'\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K-zaUTN8Y7no",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "evaluation_dsqn_random_200 = []\n",
    "\n",
    "dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "optimizer = optim.Adam(dsqn.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dsqn = DSQN(architecture, 0, alpha, beta, weight_scale, 1, threshold, simulation_time)\n",
    "    dsqn.load_state_dict(torch.load(result_dir + '/checkpoint_DSQN_{}.pt'.format(i)))\n",
    "    \n",
    "    agent = Agent(env_name, dsqn, dsqn, architecture, batch_size,\n",
    "              replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "              update_every, target_update_frequency, optimizer, learning_rate,\n",
    "              num_episodes, max_steps, 0, result_dir, 0, tau, SQN=True, two_neurons=False, random=True)\n",
    "    \n",
    "    rewards = agent.evaluate_agent(dsqn, 100, 200, gym_evaluation_seeds)\n",
    "    evaluation_dsqn_random_200.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dsqn_200', evaluation_dsqn_random_200)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dsqn_random_200)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dsqn_random_200)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oPzLzY9IY7np",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "evaluation_dqn_random_200 = []\n",
    "\n",
    "for i in best_runs:\n",
    "    print(\"Run # {}\".format(i))\n",
    "    dqn = QNetwork(architecture, 1).to(device)\n",
    "    dqn.load_state_dict(torch.load(result_dir + '/checkpoint_DQN_{}.pt'.format(i)))\n",
    "    rewards = agent.evaluate_agent(dqn, 100, 200, gym_evaluation_seeds)\n",
    "    evaluation_dqn_random_200.extend(rewards)\n",
    "    print(\"Mean Rewards: {}\".format(np.mean(rewards)))\n",
    "    print(\"Deviation: {}\".format(np.std(rewards)))\n",
    "    print(\"-----------------\")\n",
    "np.save(result_dir + '/evaluation_dqn_200', evaluation_dqn_random_200)\n",
    "print(\"Total Mean Reward: {}\".format(np.mean(evaluation_dqn_random_200)))\n",
    "print(\"Total Deviation: {}\".format(np.std(evaluation_dqn_random_200)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "klmqC7TdY7np",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "result_dir"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}